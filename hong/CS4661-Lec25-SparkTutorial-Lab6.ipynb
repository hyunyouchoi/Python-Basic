{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science (CS4661). Cal State Univ. LA, CS Dept.\n",
    "## Dr. Mo. Porhomayoun\n",
    "----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lab6: Spark Tutorial\n",
    "\n",
    "#### Feel free to refer to the suggested resources, references, and documentaries for more details:\n",
    "- http://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- http://spark.apache.org/examples.html\n",
    "- https://www.edx.org/course/big-data-analysis-apache-spark-uc-berkeleyx-cs110x\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You first need to install PySpark to be able to use Spark in Python.\n",
    "For MAC users just type one of the following in terminal:\n",
    "\n",
    "```\n",
    "brew install apache-spark\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing pyspark and defining a spark context:\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Parallelized RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['dog', 'cat', 'tiger','elephant', 'pig', 'pig', 'cat']\n",
    "\n",
    "n_partitions = 3 # number of partitions in parallel processing\n",
    "\n",
    "wordsRDD = sc.parallelize(wordsList, n_partitions) # Lazy!\n",
    "\n",
    "print(type(wordsRDD))\n",
    "print(wordsRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accessing the elements of the RDD:\n",
    "\n",
    "# first element:\n",
    "print(wordsRDD.first())\n",
    "\n",
    "# first 3 items:\n",
    "print(wordsRDD.take(3))\n",
    "\n",
    "# all items:\n",
    "print(wordsRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining an arbitrary function for map:\n",
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "print(makePlural('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['dog', 'cat', 'tiger','elephant', 'pig', 'pig', 'cat', 'cat']\n",
    "\n",
    "n_partitions = 3 # number of partitions in parallel processing\n",
    "\n",
    "wordsRDD = sc.parallelize(wordsList, n_partitions)\n",
    "\n",
    "# MAP: each machine applies the function on a chunck of data in parallel:\n",
    "pluralRDD = wordsRDD.map(makePlural)    \n",
    "\n",
    "print(pluralRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is Lazy! It has done nothing so far. It just remember the transformations for each chunck of data. The transformations are only computed when an action requires the results.\n",
    "\n",
    "RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. \n",
    "For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program  (Ref: Spark Programming Guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is an action that requests for final results\n",
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reviewing lambda:\n",
    "\n",
    "# Example1:\n",
    "# defining a function that gets an input, and calculate and return squared value using lambda\n",
    "mySqr = lambda x: x**2\n",
    "y = mySqr(2)\n",
    "print(y)\n",
    "\n",
    "# Example2:\n",
    "# defining a function that gets two input, and calculate and return the summation of squared values using lambda\n",
    "mySqrSum = lambda x1,x2: x1**2 + x2**2 \n",
    "y = mySqrSum(2,4)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-Reduce for Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- MAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['dog', 'cat', 'tiger','elephant', 'pig', 'pig', 'cat', 'cat']\n",
    "\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "# map with lambda:\n",
    "keyValueList = wordsRDD.map(lambda w: (w, 1))\n",
    "\n",
    "print(keyValueList.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- REDUCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keyValueList comes from Map Stage:\n",
    "\n",
    "wordCounts = (keyValueList\n",
    "                .reduceByKey(lambda x,y: x+y)\n",
    "                .collect())\n",
    "\n",
    "print(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary  of map-reduce for word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['dog', 'cat', 'tiger','elephant', 'pig', 'pig', 'cat', 'cat']\n",
    "\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "wordCounts = (wordsRDD\n",
    "                .map(lambda w: (w, 1))\n",
    "                .reduceByKey(lambda x,y: x+y)\n",
    "                .collect())\n",
    "\n",
    "print(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# textFlie treats a txt file as a list of lines:\n",
    "\n",
    "hamletRdd = sc.textFile(\"/Users/mpourho/Documents/CSU/Courses/CS4661/Datasets/hamlet.txt\",4)\n",
    "\n",
    "print(hamletRdd) # Lazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of the first 50 lines:\n",
    "\n",
    "print(hamletRdd.take(50)) # Action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It returns the first 50 words of the hamlet:\n",
    "\n",
    "hamletwords = hamletRdd.flatMap(lambda line: line.split(\" \")) # Lazy!\n",
    "\n",
    "# Here we use flatMap rather than map to avoid list of lists!\n",
    "                    \n",
    "print(hamletwords.take(50)) # Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After converting the Hamlet into words usig parallel processing,\n",
    "# we count the number of each word in another map-reduce level:\n",
    "\n",
    "hamletwordcounts = hamletRdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "                            .map(lambda word: (word, 1)) \\\n",
    "                            .reduceByKey(lambda a, b: a + b) \\\n",
    "                            .collect()\n",
    "            \n",
    "hamletwordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make it a little more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of stop words:\n",
    "\n",
    "stopwords=[w.strip() for w in open(\n",
    "    \"/Users/mpourho/Documents/CSU/Courses/CS4661/Datasets/stopwords.txt\").readlines()]\n",
    "\n",
    "print(stopwords[1:10]) # let's see the first 10 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of keywords (not in stopwords) sorted by frequency:\n",
    "\n",
    "hamletwordcounts = (hamletRdd.flatMap(lambda line: line.split(\" \")) # words\n",
    "                    .map(lambda word: word.strip().lower()) # lower case\n",
    "                    .filter(lambda word: word not in stopwords) # no stopwords\n",
    "                    .map(lambda word: (word, 1))\n",
    "                    .reduceByKey(lambda a, b: a + b)  \n",
    "                    .takeOrdered(20, lambda x: -x[1]) # sort by value\n",
    "                    )\n",
    "                             \n",
    "hamletwordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of keywords (not in stopwords) sorted by frequency:\n",
    "\n",
    "hamletwordcounts = (hamletRdd.flatMap(lambda line: line.split(\" \")) # words\n",
    "                    .map(lambda word: word.strip().lower()) # lower case\n",
    "                    .filter(lambda word: word not in stopwords) # no stopwords\n",
    "                    .filter(lambda word: word not in ['','ham.','hor.','king.','pol.']) # no name\n",
    "                    .map(lambda word: (word, 1))\n",
    "                    .reduceByKey(lambda a, b: a + b)  \n",
    "                    .takeOrdered(20, lambda x: -x[1]) # sort by value\n",
    "                    )\n",
    "                             \n",
    "hamletwordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference for some of the examples: Dr. Rahul Dave, Harvard University."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
